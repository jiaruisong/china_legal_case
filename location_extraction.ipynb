{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf8a522-74e4-414f-ae95-3c0009ae7251",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b76fa9-a138-4649-9b26-cae855200fe4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyahocorasick  # Install known missing dependency first\n",
    "%pip install git+https://github.com/jiaruisong/chinese_province_city_area_mapper.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4050df8-e8d0-479b-81d5-94da8b3067d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Function: location Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddebf46-239a-4121-87f2-ff4c15ef8df7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import cpca\n",
    "\n",
    "def location_extraction(location):\n",
    "    df = cpca.transform(location)\n",
    "    \n",
    "    columns_mapping = {\n",
    "        \"省\": \"Province\",\n",
    "        \"市\": \"City\",\n",
    "        \"区\": \"District\",\n",
    "        \"地址\": \"CourtLevel\",\n",
    "        \"adcode\": \"Adcode\"\n",
    "    }\n",
    "\n",
    "    # Renaming the columns\n",
    "    df_renamed = df.rename(columns=columns_mapping)\n",
    "\n",
    "    return df_renamed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd47cdba-5e6b-4c97-bc86-cfa726c4b8be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f77bee8-48fd-46f9-accc-8a5f5cb25071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "court_names = [\n",
    "    \"江苏省高级人民法院\",\n",
    "    \"株洲市芦淞区人民法院\",\n",
    "    \"湖南省高级人民法院\",\n",
    "    \"云南省曲靖市中级人民法院\",\n",
    "    \"湖南省长沙市中级人民法院\",\n",
    "    \"湖南省长沙市中级人民法院\",\n",
    "    \"上海市第二中级人民法院\",\n",
    "    \"南宁市江南区人民法院\",\n",
    "    \"不存在县人民法院\",\n",
    "    \"深圳市龙岗区人民法院\"\n",
    "]\n",
    "\n",
    "court_names_1 = [\"不存在县人民法院\"]\n",
    "df = location_extraction(court_names)\n",
    "print(df)\n",
    "df = location_extraction(court_names_1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94597abb-6b0d-42b4-bb8e-00751748fad3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Location Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31899c98-5ef1-4fc9-a46d-de8209580048",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def process_files(base_path, cause):\n",
    "    output_folder = f\"{base_path}/{cause}_Location_March_18\"\n",
    "    path_pattern = os.path.join(base_path, cause)\n",
    "\n",
    "    files = dbutils.fs.ls(path_pattern)\n",
    "\n",
    "    for file in files:\n",
    "        if file.isDir():\n",
    "            sub_files = dbutils.fs.ls(file.path)\n",
    "            for sub_file in sub_files:\n",
    "                if sub_file.name.endswith(\".csv\"):\n",
    "                    file_path = sub_file.path.replace(\"dbfs:\", \"/dbfs\")\n",
    "                    df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "                    df_filtered = df[df['TrialProcedure'].str.contains('一审', na=False)]\n",
    "\n",
    "                    if df_filtered.empty:\n",
    "                        print(f\"No data after filtering for {sub_file.name}. Moving to the next file.\")\n",
    "                        continue\n",
    "\n",
    "                    # Reset index to use as a temporary join key\n",
    "                    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                    # Apply address categorization function to the 'court' column and reset its index\n",
    "                    #address_data = df_filtered['Court'].apply(location_extraction).reset_index()\n",
    "\n",
    "                    # Merge the original DataFrame with the categorized address data\n",
    "                    #df_enriched = pd.merge(df_filtered, address_data, left_index=True, right_index=True)\n",
    "\n",
    "                    # Apply the function and get a Series of DataFrames\n",
    "                    #address_data_series = df['Court'].apply(location_extraction)\n",
    "\n",
    "                    # Convert the Series of DataFrames into a single DataFrame\n",
    "                    #address_data = pd.concat(address_data_series.tolist(), ignore_index=True)\n",
    "\n",
    "                    # Call the function with the entire 'court' column as a list\n",
    "                    address_data = location_extraction(df['Court'].tolist())\n",
    "\n",
    "                    # Ensure the resulting DataFrame has the same index as the original for correct row alignment\n",
    "                    address_data.index = df.index\n",
    "\n",
    "                    # Merge the new address data with the original DataFrame\n",
    "                    df_enriched = pd.concat([df, address_data], axis=1)\n",
    "\n",
    "                    # Construct and save the output file as before\n",
    "                    output_file_path = 'dbfs:'+ os.path.join(output_folder, f\"{os.path.basename(sub_file.name)}\")\n",
    "                    df_enriched.to_csv(output_file_path.replace(\"dbfs:\", \"/dbfs\"), index=False)\n",
    "\n",
    "\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "causes_of_action = [\"drug_related\"]\n",
    "\n",
    "for cause in causes_of_action:\n",
    "    process_files(base_path, cause)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "location_extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
