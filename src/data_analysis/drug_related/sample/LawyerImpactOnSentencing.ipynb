{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a734084-b235-42b4-b5fd-d89eb30d78ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"DrugCrimeAnalysis\").getOrCreate()\n",
    "\n",
    "\n",
    "df_infer = spark.read.option(\"header\", \"true\").option(\"multiLine\", \"true\").csv(\"/mnt/processed_data_criminal_case_analysis/drug_related_data_enrich_cleaning_Apr_15/2_lawyer_presence_data_enrich\")\n",
    "\n",
    "df_infer.printSchema()\n",
    "df_infer.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64cfa0fc-1a99-4d62-b8a3-a7466cfab3be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"DrugCrimeAnalysis\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"amount_a\", IntegerType(), True),\n",
    "    StructField(\"amount_b\", IntegerType(), True),\n",
    "    StructField(\"lawyer\", BooleanType(), True),\n",
    "    StructField(\"public_defender\", BooleanType(), True),\n",
    "    StructField(\"TotalImprisonmentLengthforCriminalA\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"/mnt/processed_data_criminal_case_analysis/drug_related_data_enrich_cleaning_Apr_15/2_lawyer_presence_data_enrich\", header=True, schema=schema)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#DEBUG:\n",
    "\n",
    "# Filter out rows where TotalImprisonmentLengthforCriminalA is not NULL\n",
    "df_filtered = df.filter(col(\"TotalImprisonmentLengthforCriminalA\").isNotNull())\n",
    "\n",
    "# Count the number of rows in the filtered DataFrame\n",
    "row_count_filtered = df_filtered.count()\n",
    "\n",
    "print(f\"Total number of rows with non-NULL TotalImprisonmentLengthforCriminalA: {row_count_filtered}\")\n",
    "\n",
    "\n",
    "# Show the first few rows of the column\n",
    "df.select(\"TotalImprisonmentLengthforCriminalA\").show()\n",
    "\n",
    "# Get descriptive statistics for the column\n",
    "df.describe(\"TotalImprisonmentLengthforCriminalA\").show()\n",
    "\n",
    "\n",
    "# Check the output of the aggregation\n",
    "max_length = df.agg({\"TotalImprisonmentLengthforCriminalA\": \"max\"}).collect()[0][0]\n",
    "print(\"Maximum Imprisonment Length:\", max_length)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37fecdc-659d-4f28-a10c-23bc4a8c9f2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"DrugCrimeAnalysis\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"amount_a\", IntegerType(), True),\n",
    "    StructField(\"amount_b\", IntegerType(), True),\n",
    "    StructField(\"lawyer\", BooleanType(), True),\n",
    "    StructField(\"public_defender\", BooleanType(), True),\n",
    "    StructField(\"TotalImprisonmentLengthforCriminalA\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"/mnt/processed_data_criminal_case_analysis/drug_related_data_enrich_cleaning_Apr_15/2_lawyer_presence_data_enrich\", header=True, schema=schema)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#DEBUG:\n",
    "\n",
    "# Filter out rows where TotalImprisonmentLengthforCriminalA is not NULL\n",
    "df_filtered = df.filter(col(\"TotalImprisonmentLengthforCriminalA\").isNotNull())\n",
    "\n",
    "# Count the number of rows in the filtered DataFrame\n",
    "row_count_filtered = df_filtered.count()\n",
    "\n",
    "print(f\"Total number of rows with non-NULL TotalImprisonmentLengthforCriminalA: {row_count_filtered}\")\n",
    "\n",
    "\n",
    "# Show the first few rows of the column\n",
    "df.select(\"TotalImprisonmentLengthforCriminalA\").show()\n",
    "\n",
    "# Get descriptive statistics for the column\n",
    "df.describe(\"TotalImprisonmentLengthforCriminalA\").show()\n",
    "\n",
    "\n",
    "# Check the output of the aggregation\n",
    "max_length = df.agg({\"TotalImprisonmentLengthforCriminalA\": \"max\"}).collect()[0][0]\n",
    "print(\"Maximum Imprisonment Length:\", max_length)\n",
    "\n",
    "\n",
    "# Define the bucketizer\n",
    "splits = list(range(0, int(df.agg({\"TotalImprisonmentLengthforCriminalA\": \"max\"}).collect()[0][0]) + 3, 3))\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"TotalImprisonmentLengthforCriminalA\", outputCol=\"ImprisonmentLengthBucket\")\n",
    "\n",
    "# Transform the data\n",
    "df_binned = bucketizer.transform(df)\n",
    "\n",
    "# Group by lawyer status and imprisonment length bucket, then count\n",
    "result_df = df_binned.groupBy(\"lawyer\", \"ImprisonmentLengthBucket\").count()\n",
    "\n",
    "# Convert the result to Pandas DataFrame for plotting (if the dataset is small enough)\n",
    "result_pandas = result_df.toPandas()\n",
    "\n",
    "# Plot using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "for label, grp in result_pandas.groupby('lawyer'):\n",
    "    grp.plot(x='ImprisonmentLengthBucket', y='count', ax=ax, label=label, kind='bar')\n",
    "plt.title('Histogram of Imprisonment Length by Lawyer Presence')\n",
    "plt.xlabel('Imprisonment Length (months)')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.xticks(ticks=range(len(splits)-1), labels=[f\"{splits[i]}-{splits[i+1]}\" for i in range(len(splits)-1)])\n",
    "plt.legend(title='Lawyer Present')\n",
    "plt.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5d0149b-40b8-47e7-bce4-bf3995960514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LawyerImpactOnSentencing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
