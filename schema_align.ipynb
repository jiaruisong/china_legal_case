{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 1: spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc8901e-1fbf-4033-bdaf-9175166a422a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/mnt/processed_data_criminal_case_analysis/drug_related_DrugTypeAmount_Penalty_Location_March_19\"\n",
    "\n",
    "output_path = \"/mnt/processed_data_criminal_case_analysis/drug_related_DrugTypeAmount_Penalty_Location_DataframeAlignment_March_20\"\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Assuming `common_columns` is a list of all columns identified\n",
    "def align_dataframe(df, common_columns):\n",
    "    for col in common_columns:\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, lit(None))\n",
    "    return df.select(common_columns)\n",
    "\n",
    "common_columns = [\n",
    "    \"OriginalLink\",\n",
    "    \"CaseNumber\",\n",
    "    \"CaseName\",\n",
    "    \"Court\",\n",
    "    \"Location\",\n",
    "    \"CaseType\",\n",
    "    \"TrialProcedure\",\n",
    "    \"JudgmentDate\",\n",
    "    \"PublicationDate\",\n",
    "    \"PartiesInvolved\",\n",
    "    \"CausesofAction\",\n",
    "    \"LegalBasis\",\n",
    "    \"FullText\",\n",
    "    \"drug_a\",\n",
    "    \"amount_a\",\n",
    "    \"drug_b\",\n",
    "    \"amount_b\",\n",
    "    \"drug_c\",\n",
    "    \"amount_c\",\n",
    "    \"ResponseText\",\n",
    "    \"Charge1forCriminalA\",\n",
    "    \"FineforCriminalA\",\n",
    "    \"TotalImprisonmentLengthforCriminalA\",\n",
    "    \"SuspendedforCriminalA\",\n",
    "    \"Charge2forCriminalA\",\n",
    "    \"Charge1forCriminalB\",\n",
    "    \"Charge2forCriminalB\",\n",
    "    \"FineforCriminalB\",\n",
    "    \"TotalImprisonmentLengthforCriminalB\",\n",
    "    \"SuspendedforCriminalB\",\n",
    "    \"Charge1forCriminalC\",\n",
    "    \"Charge2forCriminalC\",\n",
    "    \"FineforCriminalC\",\n",
    "    \"TotalImprisonmentLengthforCriminalC\",\n",
    "    \"SuspendedforCriminalC\",\n",
    "    \"Charge1forCriminalD\",\n",
    "    \"Charge2forCriminalD\",\n",
    "    \"FineforCriminalD\",\n",
    "    \"TotalImprisonmentLengthforCriminalD\",\n",
    "    \"SuspendedforCriminalD\",\n",
    "    \"Charge1forCriminalE\",\n",
    "    \"Charge2forCriminalE\",\n",
    "    \"FineforCriminalE\",\n",
    "    \"TotalImprisonmentLengthforCriminalE\",\n",
    "    \"SuspendedforCriminalE\",\n",
    "    \"Charge1forCriminalF\",\n",
    "    \"Charge2forCriminalF\",\n",
    "    \"FineforCriminalF\",\n",
    "    \"TotalImprisonmentLengthforCriminalF\",\n",
    "    \"SuspendedforCriminalF\",\n",
    "    \"Charge3forCriminalC\",\n",
    "    \"Province\",\n",
    "    \"City\",\n",
    "    \"District\",\n",
    "    \"CourtLevel\",\n",
    "    \"Adcode\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_files = dbutils.fs.ls(base_path)\n",
    "\n",
    "for sub_file in sub_files:\n",
    "    if sub_file.name.endswith(\".csv\"):\n",
    "        file_path = sub_file.path\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(file_path)\n",
    "        df_aligned = align_dataframe(df, common_columns)\n",
    "        df_aligned.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "        \n",
    "        print(f\"Aligned dataframe for {file_path}\")\n",
    "        df_aligned.printSchema()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 2: spark sql="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d953034-03a6-4daa-a12a-8cdd8db74871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temp view\n",
    "df.createOrReplaceTempView(\"cases_view\")\n",
    "\n",
    "# Execute SQL query to find non-integer values\n",
    "non_integer_values_query = \"\"\"\n",
    "SELECT OriginalLink, CaseNumber,\tCaseName,\tCourt,\tLocation,\tCaseType,\tTrialProcedure,\tJudgmentDate,PublicationDate, TotalImprisonmentLengthforCriminalA, PartiesInvolved,\tCausesofAction,\tLegalBasis,\tFullText,\tdrug_a,\tamount_a,\tdrug_b,\tamount_b,\tResponseText,\tCharge1forCriminalA\tFineforCriminalA,\tTotalImprisonmentLengthforCriminalA,\tSuspendedforCriminalA,\tCharge2forCriminalA,\tCharge1forCriminalB,\tCharge2forCriminalB\tFineforCriminalB,\tTotalImprisonmentLengthforCriminalB,\tSuspendedforCriminalB,\tProvince,\tCity,\tDistrict,\tCourtLevel,\tAdcode\n",
    "FROM cases_view\n",
    "WHERE CAST(TotalImprisonmentLengthforCriminalA AS INT) IS NULL\n",
    "AND TotalImprisonmentLengthforCriminalA IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "non_integer_values = spark.sql(non_integer_values_query)\n",
    "\n",
    "# Show the results\n",
    "non_integer_values.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e7a6b8-56ef-498c-846e-7b1f86e32ec9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Replace 'TotalImprisonmentLengthforCriminalA' with the actual column name you want to check\n",
    "# Repeat the process for other TotalImprisonmentLength columns as needed\n",
    "\n",
    "# Try casting the column to an integer type\n",
    "df_with_cast = df.withColumn(\"TotalImprisonmentLengthInt\", col(\"TotalImprisonmentLengthforCriminalB\").cast(\"int\"))\n",
    "\n",
    "# Filter to find rows where cast is not successful\n",
    "# This condition checks for nulls in the casted column which indicates unsuccessful casts\n",
    "non_integer_rows = df_with_cast.filter(df_with_cast[\"TotalImprisonmentLengthInt\"].isNull() & ~df_with_cast[\"TotalImprisonmentLengthforCriminalB\"].isNull())\n",
    "\n",
    "# Show the rows with non-integer values\n",
    "non_integer_rows.select(\"TotalImprisonmentLengthforCriminalB\").show(100, truncate=False)\n",
    "#non_integer_rows.show(non_integer_rows.count(), truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 3: pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    \"drug_c\",\n",
    "    \"amount_c\",\n",
    "    \"Charge1forCriminalC\",\n",
    "    \"Charge2forCriminalC\",\n",
    "    \"FineforCriminalC\",\n",
    "    \"TotalImprisonmentLengthforCriminalC\",\n",
    "    \"SuspendedforCriminalC\",\n",
    "    \"Charge1forCriminalD\",\n",
    "    \"Charge2forCriminalD\",\n",
    "    \"FineforCriminalD\",\n",
    "    \"TotalImprisonmentLengthforCriminalD\",\n",
    "    \"SuspendedforCriminalD\",\n",
    "    \"Charge1forCriminalE\",\n",
    "    \"Charge2forCriminalE\",\n",
    "    \"FineforCriminalE\",\n",
    "    \"TotalImprisonmentLengthforCriminalE\",\n",
    "    \"SuspendedforCriminalE\",\n",
    "    \"Charge1forCriminalF\",\n",
    "    \"Charge2forCriminalF\",\n",
    "    \"FineforCriminalF\",\n",
    "    \"TotalImprisonmentLengthforCriminalF\",\n",
    "    \"SuspendedforCriminalF\",\n",
    "    \"Charge3forCriminalC\",\n",
    "]\n",
    "\n",
    "all_columns = [\n",
    "    \"OriginalLink\",\n",
    "    \"CaseNumber\",\n",
    "    \"CaseName\",\n",
    "    \"Court\",\n",
    "    \"Location\",\n",
    "    \"CaseType\",\n",
    "    \"TrialProcedure\",\n",
    "    \"JudgmentDate\",\n",
    "    \"PublicationDate\",\n",
    "    \"PartiesInvolved\",\n",
    "    \"CausesofAction\",\n",
    "    \"LegalBasis\",\n",
    "    \"FullText\",\n",
    "    \"drug_a\",\n",
    "    \"amount_a\",\n",
    "    \"drug_b\",\n",
    "    \"amount_b\",\n",
    "    \"ResponseText\",\n",
    "    \"Charge1forCriminalA\",\n",
    "    \"FineforCriminalA\",\n",
    "    \"TotalImprisonmentLengthforCriminalA\",\n",
    "    \"SuspendedforCriminalA\",\n",
    "    \"Charge2forCriminalA\",\n",
    "    \"Charge1forCriminalB\",\n",
    "    \"Charge2forCriminalB\",\n",
    "    \"FineforCriminalB\",\n",
    "    \"TotalImprisonmentLengthforCriminalB\",\n",
    "    \"SuspendedforCriminalB\",\n",
    "    \"Province\",\n",
    "    \"City\",\n",
    "    \"District\",\n",
    "    \"CourtLevel\",\n",
    "    \"Adcode\"\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check unique values before moving to datacleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def align_dataframe(base_path, cause):\n",
    "\n",
    "\n",
    "    # Using dbutils.fs.ls to list directories/files\n",
    "    files = dbutils.fs.ls(base_path)\n",
    "\n",
    "    for sub_file in files:\n",
    "        if sub_file.name.endswith(\".csv\"):\n",
    "            # Reading CSV file into DataFrame\n",
    "            # Convert to local file path if necessary\n",
    "            file_path = sub_file.path.replace(\"dbfs:\", \"/dbfs\")\n",
    "            df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "            \n",
    "            for x in ['A', 'B', 'C', 'D', 'E', 'F']:\n",
    "                try:\n",
    "                    print(df[f'TotalImprisonmentLengthforCriminal{x}'].unique())\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            \n",
    "\n",
    "\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis/drug_related_DrugTypeAmount_Penalty_Location_March_19\"\n",
    "\n",
    "causes_of_action = [\"drug_related\"]\n",
    "\n",
    "for cause in causes_of_action:\n",
    "    align_dataframe(base_path, cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def align_dataframe(base_path, cause):\n",
    "\n",
    "    # Define the output folder based on the base path and cause\n",
    "    output_folder = \"mnt/processed_data_criminal_case_analysis/drug_related_DrugTypeAmount_Penalty_Location_DataframeAlignment_March_20\"\n",
    "\n",
    "    # Using dbutils.fs.ls to list directories/files\n",
    "    files = dbutils.fs.ls(base_path)\n",
    "\n",
    "    for sub_file in files:\n",
    "        if sub_file.name.endswith(\".csv\"):\n",
    "            # Reading CSV file into DataFrame\n",
    "            # Convert to local file path if necessary\n",
    "            file_path = sub_file.path.replace(\"dbfs:\", \"/dbfs\")\n",
    "            df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "            \n",
    "            #drop columns that are not needed\n",
    "            df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "            \n",
    "            # add missing columns\n",
    "            for column in all_columns:\n",
    "                if column not in df.columns:\n",
    "                    df[column] = None\n",
    "            \n",
    "            #data cleaning\n",
    "            # fulltext empty: drop all row\n",
    "            \n",
    "            # replace '无期徒刑' with 99999\n",
    "            df['TotalImprisonmentLength'].replace({'无期徒刑': 99999}, inplace=True)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            # Construct the output path for the enriched CSV file\n",
    "            output_file_path = 'dbfs:' + \\\n",
    "                os.path.join(\n",
    "                    output_folder, f\"{os.path.basename(sub_file.name)}\")\n",
    "\n",
    "            # Save the processed DataFrame to the new CSV file, ensuring the path is in \"/dbfs\" format for local IO\n",
    "            df_enriched.to_csv(output_file_path.replace(\n",
    "                \"dbfs:\", \"/dbfs\"), index=False)\n",
    "\n",
    "\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis/drug_related_DrugTypeAmount_Penalty_Location_March_19\"\n",
    "\n",
    "causes_of_action = [\"drug_related\"]\n",
    "\n",
    "for cause in causes_of_action:\n",
    "    align_dataframe(base_path, cause)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read_schema",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
