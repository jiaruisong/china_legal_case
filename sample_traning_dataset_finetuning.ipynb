{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "this code is to sample another 0.06 % of 刑事一审 cases from the four course_of_actions, in order to assemble a finetuning training dataset for LLM, to better extract information from the judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36217b79-d045-4c9e-9fa8-369191b087e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# assemble folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ec8092-5912-44ab-a3fc-cf1437a16b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/mnt/blob_berkeley_account\"\n",
    "folder_names = [f\"{base_path}/{year}_judgment_data\" for year in range(2010, 2022)] \n",
    "\n",
    "def list_csv_files(path):\n",
    "    \"\"\"\n",
    "    List all CSV files recursively within a given path.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.path.endswith(\".csv\"):\n",
    "            files.append(item.path)\n",
    "        elif item.isDir():  # If the item is a directory, recurse\n",
    "            files.extend(list_csv_files(item.path))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b89cc52-1006-4a0d-b773-e4921f5d75e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "select relevant columns, rename them into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc909ad-c1ed-4b02-bf2e-eb59702490b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    \"原始链接\": \"OriginalLink\",\n",
    "    \"案号\": \"CaseNumber\",\n",
    "    \"案件名称\": \"CaseName\",\n",
    "    \"法院\": \"Court\",\n",
    "    \"所属地区\": \"Location\",\n",
    "    \"案件类型\": \"CaseType\",\n",
    "    \"审理程序\": \"TrialProcedure\",\n",
    "    \"裁判日期\": \"JudgmentDate\",\n",
    "    \"公开日期\": \"PublicationDate\",\n",
    "    \"当事人\": \"PartiesInvolved\",\n",
    "    \"案由\": \"CausesofAction\",\n",
    "    \"法律依据\": \"LegalBasis\",\n",
    "    \"全文\": \"FullText\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "617879f1-6db4-4860-8336-2229ef9e443d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Select columns, rename, filter out CaseReasons, them sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78272e7e-b730-48ee-98b1-66f22d79451d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Causes of Action to filter and sample\n",
    "causes_of_action = {\n",
    "    \"盗窃\": \"theft\",\n",
    "    \"交通肇事\": \"traffic_accidents\",\n",
    "    \"毒品\": \"drug_related\",\n",
    "    \"诈骗\": \"fraud\"\n",
    "}\n",
    "\n",
    "sample_fraction = 0.0006 \n",
    "\n",
    "import re\n",
    "\n",
    "for folder in folder_names:\n",
    "    csv_files = list_csv_files(folder)\n",
    "    for file_path in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Select and rename the relevant columns\n",
    "        df_selected_and_renamed = df.select([col(c).alias(column_mapping[c]) for c in column_mapping.keys()])\n",
    "        \n",
    "        # Filter to include only cases that are 刑事一审 in CaseType\n",
    "        df_criminal_cases = df_selected_and_renamed.filter(col(\"CaseType\").contains(\"刑事\"))\n",
    "        df_criminal_cases_first_trail = df_criminal_cases.filter(col(\"TrialProcedure\").contains(\"一审\"))\n",
    "        \n",
    "        # Loop through each cause of action, filter, sample, and process/save\n",
    "        for cause, output_suffix in causes_of_action.items():\n",
    "            df_filtered = df_criminal_cases.filter(col(\"CausesofAction\").contains(cause))\n",
    "            sampled_df = df_filtered.sample(fraction=sample_fraction)  # Sample the filtered DataFrame\n",
    "\n",
    "            # Extract year and month from file_path\n",
    "            match = re.search(r'(\\d{4})_(\\d{2})_judgment\\.csv', file_path)\n",
    "            if match:\n",
    "                year = match.group(1)  # '2013'\n",
    "                month = match.group(2)  # '01'\n",
    "            else:\n",
    "                raise ValueError(f\"Month pattern not found in the file path: {file_path}\")\n",
    "\n",
    "            saved_file_name = f\"{year}_{month}_{output_suffix}_judgment_data\"\n",
    "            \n",
    "            # Define the full path for saving the DataFrame\n",
    "            full_path = f\"/mnt/processed_data_criminal_case_analysis/finetuning_training_data/{output_suffix}/{saved_file_name}\"\n",
    "            \n",
    "            # Save the DataFrame\n",
    "            sampled_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(full_path)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how many 一审刑事案件 are there after sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CountCases\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Causes of Action to filter and sample\n",
    "causes_of_action = {\n",
    "    \"盗窃\": \"theft\",\n",
    "    \"交通肇事\": \"traffic_accidents\",\n",
    "    \"毒品\": \"drug_related\",\n",
    "    \"诈骗\": \"fraud\"\n",
    "}\n",
    "\n",
    "# Dictionary to hold the count of cases for each cause of action\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames and count the cases\n",
    "for cause, output_suffix in causes_of_action.items():\n",
    "    # Construct the path to read the saved files for the current cause of action\n",
    "    path = f\"/mnt/processed_data_criminal_case_analysis/finetuning_training_data/{output_suffix}/*/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Count the number of cases for the current cause of action\n",
    "    count = df.count()\n",
    "    \n",
    "    # Store the count in the dictionary\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"{cause}案件数: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the output into one csv for data labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AggregateCases\").getOrCreate()\n",
    "\n",
    "# Base path for reading and saving data\n",
    "read_base_path = \"/mnt/processed_data_criminal_case_analysis/finetuning_training_data\"\n",
    "save_base_path = \"/mnt/processed_data_criminal_case_analysis/finetuning_training_data/aggregated_data\"\n",
    "\n",
    "# Causes of Action to filter and sample\n",
    "causes_of_action = {\n",
    "    \"盗窃\": \"theft\",\n",
    "    \"交通肇事\": \"traffic_accidents\",\n",
    "    \"毒品\": \"drug_related\",\n",
    "    \"诈骗\": \"fraud\"\n",
    "}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames, combine them, and write back to a single CSV\n",
    "for cause, output_suffix in causes_of_action.items():\n",
    "    # Construct the path to read the saved files for the current cause of action\n",
    "    read_path = f\"{read_base_path}/{output_suffix}/*/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(read_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Define the path for saving the combined CSV\n",
    "    save_path = f\"{save_base_path}/{output_suffix}.csv\"\n",
    "    \n",
    "    # Save the combined data to a single CSV file\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Analysis - Filter & Sample",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
