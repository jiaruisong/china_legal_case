{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31adbc0-7b81-4ae9-bd7a-08774322583d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "how many cases are there after sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe71c52b-cbae-46e2-a9ae-9f32217ebb50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "盗窃案件数: 11750\n交通肇事案件数: 3635\n毒品案件数: 6812\n诈骗案件数: 4514\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CountCases\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Causes of Action to filter and sample\n",
    "causes_of_action = {\n",
    "    \"盗窃\": \"theft\",\n",
    "    \"交通肇事\": \"traffic_accidents\",\n",
    "    \"毒品\": \"drug_related\",\n",
    "    \"诈骗\": \"fraud\"\n",
    "}\n",
    "\n",
    "# Dictionary to hold the count of cases for each cause of action\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames and count the cases\n",
    "for cause, output_suffix in causes_of_action.items():\n",
    "    # Construct the path to read the saved files for the current cause of action\n",
    "    path = f\"{base_path}/{output_suffix}/*/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Count the number of cases for the current cause of action\n",
    "    count = df.count()\n",
    "    \n",
    "    # Store the count in the dictionary\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"{cause}案件数: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bac5527-a6e8-464a-be69-6696cc85794d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "how many 一审 cases are there after the sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f5c2b0-b1f3-4fb5-b6ba-3b1630e88d4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一审的盗窃案件数: 8543\n一审的交通肇事案件数: 3102\n一审的毒品案件数: 2851\n一审的诈骗案件数: 2656\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CountCases\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Causes of Action to filter and sample\n",
    "causes_of_action = {\n",
    "    \"盗窃\": \"theft\",\n",
    "    \"交通肇事\": \"traffic_accidents\",\n",
    "    \"毒品\": \"drug_related\",\n",
    "    \"诈骗\": \"fraud\"\n",
    "}\n",
    "\n",
    "# Dictionary to hold the count of cases for each cause of action\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames and count the cases\n",
    "for cause, output_suffix in causes_of_action.items():\n",
    "    # Construct the path to read the saved files for the current cause of action\n",
    "    path = f\"{base_path}/{output_suffix}/*/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    df_filtered = df.filter(col(\"TrialProcedure\").contains(\"一审\"))\n",
    "    \n",
    "    # Count the number of cases for the current cause of action\n",
    "    count = df_filtered.count()\n",
    "    \n",
    "    # Store the count in the dictionary\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"一审的{cause}案件数: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af0bd63f-9fe2-4bc3-944e-ef9752cb3574",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "how many二审案件？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73131f6b-7ca1-45ee-b23f-a43c3608f55e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二审的盗窃案件数: 679\n二审的交通肇事案件数: 270\n二审的毒品案件数: 585\n二审的诈骗案件数: 593\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CountCases\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Causes of Action to filter and sample\n",
    "causes_of_action = {\n",
    "    \"盗窃\": \"theft\",\n",
    "    \"交通肇事\": \"traffic_accidents\",\n",
    "    \"毒品\": \"drug_related\",\n",
    "    \"诈骗\": \"fraud\"\n",
    "}\n",
    "\n",
    "# Dictionary to hold the count of cases for each cause of action\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames and count the cases\n",
    "for cause, output_suffix in causes_of_action.items():\n",
    "    # Construct the path to read the saved files for the current cause of action\n",
    "    path = f\"{base_path}/{output_suffix}/*/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    df_filtered = df.filter(col(\"TrialProcedure\").contains(\"二审\"))\n",
    "    \n",
    "    # Count the number of cases for the current cause of action\n",
    "    count = df_filtered.count()\n",
    "    \n",
    "    # Store the count in the dictionary\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"二审的{cause}案件数: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a090b7a4-1e55-4bf0-9737-33bea38a0347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"my_managed_discovery_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Discovery - how many cases are there",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
