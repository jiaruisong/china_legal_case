{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc8901e-1fbf-4033-bdaf-9175166a422a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/mnt/processed_data_criminal_case_analysis/drug_related_DrugTypeAmount_Penalty_Location_March_19\"\n",
    "\n",
    "output_path = \"/mnt/processed_data_criminal_case_analysis/drug_related_DrugTypeAmount_Penalty_Location_DataframeAlignment_March_20\"\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Assuming `common_columns` is a list of all columns identified\n",
    "def align_dataframe(df, common_columns):\n",
    "    for col in common_columns:\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, lit(None))\n",
    "    return df.select(common_columns)\n",
    "\n",
    "common_columns = [\n",
    "    \"OriginalLink\",\n",
    "    \"CaseNumber\",\n",
    "    \"CaseName\",\n",
    "    \"Court\",\n",
    "    \"Location\",\n",
    "    \"CaseType\",\n",
    "    \"TrialProcedure\",\n",
    "    \"JudgmentDate\",\n",
    "    \"PublicationDate\",\n",
    "    \"PartiesInvolved\",\n",
    "    \"CausesofAction\",\n",
    "    \"LegalBasis\",\n",
    "    \"FullText\",\n",
    "    \"drug_a\",\n",
    "    \"amount_a\",\n",
    "    \"drug_b\",\n",
    "    \"amount_b\",\n",
    "    \"drug_c\",\n",
    "    \"amount_c\",\n",
    "    \"ResponseText\",\n",
    "    \"Charge1forCriminalA\",\n",
    "    \"FineforCriminalA\",\n",
    "    \"TotalImprisonmentLengthforCriminalA\",\n",
    "    \"SuspendedforCriminalA\",\n",
    "    \"Charge2forCriminalA\",\n",
    "    \"Charge1forCriminalB\",\n",
    "    \"Charge2forCriminalB\",\n",
    "    \"FineforCriminalB\",\n",
    "    \"TotalImprisonmentLengthforCriminalB\",\n",
    "    \"SuspendedforCriminalB\",\n",
    "    \"Charge1forCriminalC\",\n",
    "    \"Charge2forCriminalC\",\n",
    "    \"FineforCriminalC\",\n",
    "    \"TotalImprisonmentLengthforCriminalC\",\n",
    "    \"SuspendedforCriminalC\",\n",
    "    \"Charge1forCriminalD\",\n",
    "    \"Charge2forCriminalD\",\n",
    "    \"FineforCriminalD\",\n",
    "    \"TotalImprisonmentLengthforCriminalD\",\n",
    "    \"SuspendedforCriminalD\",\n",
    "    \"Charge1forCriminalE\",\n",
    "    \"Charge2forCriminalE\",\n",
    "    \"FineforCriminalE\",\n",
    "    \"TotalImprisonmentLengthforCriminalE\",\n",
    "    \"SuspendedforCriminalE\",\n",
    "    \"Charge1forCriminalF\",\n",
    "    \"Charge2forCriminalF\",\n",
    "    \"FineforCriminalF\",\n",
    "    \"TotalImprisonmentLengthforCriminalF\",\n",
    "    \"SuspendedforCriminalF\",\n",
    "    \"Charge3forCriminalC\",\n",
    "    \"Province\",\n",
    "    \"City\",\n",
    "    \"District\",\n",
    "    \"CourtLevel\",\n",
    "    \"Adcode\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_files = dbutils.fs.ls(base_path)\n",
    "\n",
    "for sub_file in sub_files:\n",
    "    if sub_file.name.endswith(\".csv\"):\n",
    "        file_path = sub_file.path\n",
    "        df = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .load(file_path)\n",
    "        df_aligned = align_dataframe(df, common_columns)\n",
    "        df_aligned.write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "        \n",
    "        print(f\"Aligned dataframe for {file_path}\")\n",
    "        df_aligned.printSchema()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d953034-03a6-4daa-a12a-8cdd8db74871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temp view\n",
    "df.createOrReplaceTempView(\"cases_view\")\n",
    "\n",
    "# Execute SQL query to find non-integer values\n",
    "non_integer_values_query = \"\"\"\n",
    "SELECT OriginalLink, CaseNumber,\tCaseName,\tCourt,\tLocation,\tCaseType,\tTrialProcedure,\tJudgmentDate,PublicationDate, TotalImprisonmentLengthforCriminalA, PartiesInvolved,\tCausesofAction,\tLegalBasis,\tFullText,\tdrug_a,\tamount_a,\tdrug_b,\tamount_b,\tResponseText,\tCharge1forCriminalA\tFineforCriminalA,\tTotalImprisonmentLengthforCriminalA,\tSuspendedforCriminalA,\tCharge2forCriminalA,\tCharge1forCriminalB,\tCharge2forCriminalB\tFineforCriminalB,\tTotalImprisonmentLengthforCriminalB,\tSuspendedforCriminalB,\tProvince,\tCity,\tDistrict,\tCourtLevel,\tAdcode\n",
    "FROM cases_view\n",
    "WHERE CAST(TotalImprisonmentLengthforCriminalA AS INT) IS NULL\n",
    "AND TotalImprisonmentLengthforCriminalA IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "non_integer_values = spark.sql(non_integer_values_query)\n",
    "\n",
    "# Show the results\n",
    "non_integer_values.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e7a6b8-56ef-498c-846e-7b1f86e32ec9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Replace 'TotalImprisonmentLengthforCriminalA' with the actual column name you want to check\n",
    "# Repeat the process for other TotalImprisonmentLength columns as needed\n",
    "\n",
    "# Try casting the column to an integer type\n",
    "df_with_cast = df.withColumn(\"TotalImprisonmentLengthInt\", col(\"TotalImprisonmentLengthforCriminalB\").cast(\"int\"))\n",
    "\n",
    "# Filter to find rows where cast is not successful\n",
    "# This condition checks for nulls in the casted column which indicates unsuccessful casts\n",
    "non_integer_rows = df_with_cast.filter(df_with_cast[\"TotalImprisonmentLengthInt\"].isNull() & ~df_with_cast[\"TotalImprisonmentLengthforCriminalB\"].isNull())\n",
    "\n",
    "# Show the rows with non-integer values\n",
    "non_integer_rows.select(\"TotalImprisonmentLengthforCriminalB\").show(100, truncate=False)\n",
    "#non_integer_rows.show(non_integer_rows.count(), truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read_schema",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
