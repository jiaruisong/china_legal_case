{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "sample 10 % of drug_related cases from the full_dataset on drug_related. store them in datalake gen 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36217b79-d045-4c9e-9fa8-369191b087e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "assemble folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ec8092-5912-44ab-a3fc-cf1437a16b3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/mnt/processed_data_criminal_case_analysis/drug_related_full_dataset/\"\n",
    "\n",
    "\n",
    "def list_csv_files(path):\n",
    "    \"\"\"\n",
    "    List all CSV files recursively within a given path.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.path.endswith(\".csv\"):\n",
    "            files.append(item.path)\n",
    "        elif item.isDir():  # If the item is a directory, recurse\n",
    "            files.extend(list_csv_files(item.path))\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b89cc52-1006-4a0d-b773-e4921f5d75e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "select relevant columns, rename them into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fc909ad-c1ed-4b02-bf2e-eb59702490b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    \"原始链接\": \"OriginalLink\",\n",
    "    \"案号\": \"CaseNumber\",\n",
    "    \"案件名称\": \"CaseName\",\n",
    "    \"法院\": \"Court\",\n",
    "    \"所属地区\": \"Location\",\n",
    "    \"案件类型\": \"CaseType\",\n",
    "    \"审理程序\": \"TrialProcedure\",\n",
    "    \"裁判日期\": \"JudgmentDate\",\n",
    "    \"公开日期\": \"PublicationDate\",\n",
    "    \"当事人\": \"PartiesInvolved\",\n",
    "    \"案由\": \"CausesofAction\",\n",
    "    \"法律依据\": \"LegalBasis\",\n",
    "    \"全文\": \"FullText\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "617879f1-6db4-4860-8336-2229ef9e443d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Select columns, rename, filter out CaseReasons, them sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78272e7e-b730-48ee-98b1-66f22d79451d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "sample_fraction = 0.1\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "csv_files = list_csv_files(base_path)\n",
    "for file_path in csv_files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Loop through each cause of action, filter, sample, and process/save\n",
    "    sampled_df = df.sample(fraction=sample_fraction)  # Sample the filtered DataFrame\n",
    "\n",
    "    # Extract year and month from file_path\n",
    "    match = re.search(r'(\\d{4})_(\\d{2})_drug_related_judgment_data', file_path)\n",
    "    if match:\n",
    "        year = match.group(1)  # '2013'\n",
    "        month = match.group(2)  # '01'\n",
    "    else:\n",
    "        raise ValueError(f\"Month pattern not found in the file path: {file_path}\")\n",
    "\n",
    "    saved_file_name = f\"{year}_{month}_drug_related_judgment_data\"\n",
    "    \n",
    "    # Define the full path for saving the DataFrame\n",
    "    full_path = f\"/mnt/processed_data_criminal_case_analysis/drug_related_10_percent/{saved_file_name}\"\n",
    "    \n",
    "    # Save the DataFrame\n",
    "    sampled_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(full_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a68447-ca35-4871-811e-4fdc76f3be47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test code for the a small data set\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d03bc24-d0ef-4a40-847a-355559f93051",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/processed_data_criminal_case_analysis/2013_judgment_data/</td><td>2013_judgment_data/</td><td>0</td><td>1710477564000</td></tr><tr><td>dbfs:/mnt/processed_data_criminal_case_analysis/testcsv.csv</td><td>testcsv.csv</td><td>23</td><td>1710463876000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/processed_data_criminal_case_analysis/2013_judgment_data/",
         "2013_judgment_data/",
         0,
         1710477564000
        ],
        [
         "dbfs:/mnt/processed_data_criminal_case_analysis/testcsv.csv",
         "testcsv.csv",
         23,
         1710463876000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-4495737001346625>, line 3\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# checking the mount status\u001b[39;00m\n",
       "\u001b[1;32m      2\u001b[0m display(dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/processed_data_criminal_case_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\u001b[0;32m----> 3\u001b[0m display(dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/processed_data_criminal_case_analysis/2010_judgment_data\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    360\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m    361\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
       "\n",
       "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o428.ls.\n",
       ": java.io.FileNotFoundException: No such file or directory /mnt/processed_data_criminal_case_analysis/2010_judgment_data\n",
       "\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:135)\n",
       "\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n",
       "\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:115)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:636)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:636)\n",
       "\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:115)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\nFile \u001b[0;32m<command-4495737001346625>, line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# checking the mount status\u001b[39;00m\n\u001b[1;32m      2\u001b[0m display(dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/processed_data_criminal_case_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m display(dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/processed_data_criminal_case_analysis/2010_judgment_data\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    361\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n\n\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o428.ls.\n: java.io.FileNotFoundException: No such file or directory /mnt/processed_data_criminal_case_analysis/2010_judgment_data\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$2(DatabricksFileSystemV2.scala:135)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.$anonfun$listStatus$1(DatabricksFileSystemV2.scala:115)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:636)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:636)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:636)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:636)\n\tat com.databricks.backend.daemon.data.client.DBFSV2.listStatus(DatabricksFileSystemV2.scala:115)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:164)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:254)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$2(DBUtilsCore.scala:223)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$1(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:221)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:211)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:666)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:684)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:661)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:571)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:540)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:211)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "java.io.FileNotFoundException: No such file or directory /mnt/processed_data_criminal_case_analysis/2010_judgment_data",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checking the mount status\n",
    "display(dbutils.fs.ls(\"/mnt/processed_data_criminal_case_analysis\"))\n",
    "display(dbutils.fs.ls(\"/mnt/processed_data_criminal_case_analysis/2013_judgment_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d165d93-9a97-4a76-9eb3-08670fe324a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import re\n",
    "\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "folder_names = [\"/mnt/processed_data_criminal_case_analysis/2013_judgment_data\"]\n",
    "\n",
    "def list_csv_files(path):\n",
    "    \"\"\"\n",
    "    List all CSV files recursively within a given path.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for item in dbutils.fs.ls(path):\n",
    "        if item.path.endswith(\".csv\"):\n",
    "            files.append(item.path)\n",
    "        elif item.isDir():  # If the item is a directory, recurse\n",
    "            files.extend(list_csv_files(item.path))\n",
    "    return files\n",
    "\n",
    "# Causes of Action to filter and sample\n",
    "causes_of_action = {\n",
    "    \n",
    "    \"盗窃\": \"theft\",\n",
    "    \"交通肇事\": \"traffic_accidents\",\n",
    "    \"毒品\": \"drug_related\",\n",
    "    \"诈骗\": \"fraud\"\n",
    "}\n",
    "\n",
    "sample_fraction = 0.01\n",
    "\n",
    "for folder in folder_names:\n",
    "    csv_files = list_csv_files(folder)\n",
    "    for file_path in csv_files:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Select and rename the relevant columns\n",
    "        df_selected_and_renamed = df.select([col(c).alias(column_mapping[c]) for c in column_mapping.keys()])\n",
    "        \n",
    "        # Filter to include only cases that mention \"刑事\" in CaseType\n",
    "        df_criminal_cases = df_selected_and_renamed.filter(col(\"CaseType\").contains(\"刑事\"))\n",
    "        \n",
    "        # Loop through each cause of action, filter, sample, and process/save\n",
    "        for cause, output_suffix in causes_of_action.items():\n",
    "            df_filtered = df_criminal_cases.filter(col(\"CausesofAction\").contains(cause))\n",
    "            sampled_df = df_filtered.sample(fraction=sample_fraction)  # Sample the filtered DataFrame\n",
    "\n",
    "            # Extract year and month from file_path\n",
    "            match = re.search(r'(\\d{4})年(\\d{2})月裁判文书数据\\.csv', file_path)\n",
    "            if match:\n",
    "                year = match.group(1)  # '2013'\n",
    "                month = match.group(2)  # '01'\n",
    "            else:\n",
    "                raise ValueError(\"Month pattern not found in the file path.\")\n",
    "\n",
    "            saved_file_name = f\"{year}_{month}_{output_suffix}_judgment_data\"\n",
    "            \n",
    "            # Define the full path for saving the DataFrame\n",
    "            full_path = f\"/mnt/processed_data_criminal_case_analysis/{output_suffix}/{saved_file_name}\"\n",
    "            \n",
    "            # Save the DataFrame\n",
    "            sampled_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(full_path)\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Analysis - Filter & Sample",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
