{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = dbutils.secrets.get(scope = \"OhMyGPTAPI\", key = \"OhMyGPTAPI\")\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Trim to Relevant Part - Case Type Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"trimmed_text\", StringType(), nullable=True),\n",
    "    StructField(\"trimmedType\", StringType(), nullable=True),\n",
    "    StructField(\"text_around_trimmed_point\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "@udf(returnType=schema)\n",
    "def trim_judgment(full_text):\n",
    "    \"\"\"\n",
    "    Trims judicial text to save LLM token.\n",
    "\n",
    "    If \"查明\" is found, it trims before this keyword and returns the rest. If not found, it trims away the first third\n",
    "    of the text, returning the remainder. Additionally, it returns the trimming type and twenty characters around the\n",
    "    trimming point for diagnosis.\n",
    "\n",
    "    Returns:\n",
    "    - trimmed_text (str): The result of the trimming operation.\n",
    "    - trimmedType (str): The method of trimming ('查明' or 'last 2/3').\n",
    "    - text_around_trimmed_point (str): Context around the point of trimming.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if full_text is None:\n",
    "        return None, \"ERROR\", \"\"\n",
    "    \n",
    "    # Search for \"查明\" from the end towards the beginning\n",
    "    index = full_text.rfind(\"查明\")\n",
    "    \n",
    "    if index != -1:\n",
    "        # If \"查明\" is found, trim the text before \"查明\"\n",
    "        trimmed_text = full_text[index + 2:]  # +2 to exclude \"查明\" itself\n",
    "        trimmedType = \"查明\"\n",
    "        # Calculate the position to start capturing text around \"查明\" safely\n",
    "        start_pos = max(0, index - 10)  # Start from 10 chars before \"查明\", if possible\n",
    "        end_pos = min(len(full_text), index + 10 + 2)  # Capture up to 10 chars after \"查明\", safely\n",
    "        text_around_trimmed_point = full_text[start_pos:end_pos]\n",
    "    else:\n",
    "        # If \"查明\" is not found, trim the first third of the text\n",
    "        one_third_length = len(full_text) // 3\n",
    "        trimmed_text = full_text[one_third_length:]\n",
    "        trimmedType = \"last 2/3\"\n",
    "        # Determine the break point and safely capture twenty characters around it\n",
    "        start_pos = max(0, one_third_length - 10)  # Start from 10 chars before the break point, if possible\n",
    "        end_pos = min(len(full_text), one_third_length + 10)  # Capture up to 10 chars after the break point, safely\n",
    "        text_around_trimmed_point = full_text[start_pos:end_pos]\n",
    "    \n",
    "    return trimmed_text, trimmedType, text_around_trimmed_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: Post Request and fetch Claude Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define the schema for the UDF's return type\n",
    "schema = StructType([\n",
    "    StructField(\"response_text\", StringType(), nullable=True),\n",
    "    StructField(\"trimmedType\", StringType(), nullable=True),\n",
    "    StructField(\"text_around_trimmed_point\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "@udf(returnType=schema)\n",
    "def trim_and_fetch_facts(judgment_text):\n",
    "    # Load the API key from an environment variable\n",
    "    api_key = dbutils.secrets.get(scope = \"<scope-name>\", key = \"<secret-key>\")\n",
    "    \n",
    "    trimmed_judgment_text, trimmedType, text_around_trimmed_point = trim_judgment(judgment_text)\n",
    "\n",
    "    url = \"https://api.ohmygpt.com/v1/messages\"\n",
    "    payload = json.dumps({\n",
    "        \"model\": \"claude-3-haiku-20240307\",\n",
    "        \"stream\": False,\n",
    "        \"system\": \"user will give you a legal judgement in Chinese, and you need to extract the kind and amount of drugs involved in the case. only reply using this format, '<kind of drug A>, <amount (in grams)>; <kind of drug B>, <amount(in grams)>'.  If you can't extract the kind of drug or amount. reply 'NA'. reply using Chinese.\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": trimmed_judgment_text}],\n",
    "        \"max_tokens\": 4096\n",
    "    })\n",
    "    headers = {\n",
    "        \"Authorization\": 'Bearer ' + api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, data=payload, timeout=10)\n",
    "            response.raise_for_status()  # Raises an exception for 4XX or 5XX errors\n",
    "            response_text = json.loads(response.text)[\"content\"][0][\"text\"]\n",
    "            return response_text, trimmedType, text_around_trimmed_point\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTPError on attempt {attempt+1}: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"RequestException on attempt {attempt+1}: {e}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError on attempt {attempt+1}: {e}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError on attempt {attempt+1}: {e}\")\n",
    "\n",
    "        # Wait a bit before retrying (simple exponential backoff)\n",
    "        time.sleep(2 ** attempt)\n",
    "\n",
    "    # Default values in case of failure\n",
    "    return \"Failed to fetch facts from API\", \"Error\", \"N/A\"\n",
    "\n",
    "# Remember to define the trim_judgment function as well if not already defined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ExtractFacts\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Define the set of Causes of Action to filter and sample\n",
    "causes_of_action = {\"drug_related\"}\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames, filter, and count the cases\n",
    "for cause in causes_of_action:\n",
    "    path = f\"{base_path}/{cause}/*/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Filter to get rows where TrialProcedure is '一审'\n",
    "    df_filtered = df.filter(col(\"TrialProcedure\").contains(\"一审\"))\n",
    "    \n",
    "    # Apply the UDF to the DataFrame and extract results into separate columns\n",
    "    df_processed = df_filtered.withColumn(\"processed\", trim_and_fetch_facts(df_filtered.FullText))\n",
    "\n",
    "    df_final = df_processed.select(\n",
    "        \"*\",  # Keep existing columns\n",
    "        col(\"processed.response_text\").alias(\"response_text\"),\n",
    "        col(\"processed.trimmedType\").alias(\"trimmedType\"),\n",
    "        col(\"processed.text_around_trimmed_point\").alias(\"text_around_trimmed_point\")\n",
    "    ).drop(\"processed\")  # Drop the 'processed' struct column\n",
    "    \n",
    "    # Write the modified DataFrame back to a CSV, preserving the original structure\n",
    "    output_path = f\"{base_path}/{cause}_DrugTypeAmount_March_15\"\n",
    "    df_final.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "    \n",
    "    # Count the number of cases for the current cause of action and update the dictionary\n",
    "    count = df_filtered.count()\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"{cause} modified case count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: read a small csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CountCases\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Define the set of Causes of Action to filter and sample\n",
    "causes_of_action = {\"drug_related\"}\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames, filter, and count the cases\n",
    "for cause in causes_of_action:\n",
    "    # Construct the path to read the saved files for the current cause of action\n",
    "    path = f\"{base_path}/{cause}/2015_12_drug_related_judgment_data/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Filter to get rows where TrialProcedure is '一审'\n",
    "    df_filtered = df.filter(col(\"TrialProcedure\").contains(\"一审\"))\n",
    "    \n",
    "    # Apply the UDF to the DataFrame and extract results into separate columns\n",
    "    df_processed = df_filtered.withColumn(\"processed\", trim_and_fetch_facts(df_filtered.FullText))\n",
    "\n",
    "    df_final = df_processed.select(\n",
    "        \"*\",  # Keep existing columns\n",
    "        \"processed.response_text\",\n",
    "        \"processed.trimmedType\",\n",
    "        \"processed.text_around_trimmed_point\"\n",
    "    )\n",
    "    \n",
    "    # Write the modified DataFrame back to a CSV, preserving the original structure\n",
    "    output_path = f\"{base_path}/{cause}_DrugTypeAmount_March_15\"\n",
    "    df_final.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "    \n",
    "    # Count the number of cases for the current cause of action and update the dictionary\n",
    "    count = df_filtered.count()\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"{cause} modified case count: {count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
