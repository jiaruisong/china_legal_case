{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b242eae5-6410-433a-adf7-93e07cf8d858",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Function: Trim to Relevant Part - Case Type Specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb559417-8069-4fca-b787-1bb9d3b0690a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"trimmed_text\", StringType(), nullable=True),\n",
    "    StructField(\"trimmedType\", StringType(), nullable=True),\n",
    "    StructField(\"text_around_trimmed_point\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                    # Append the enriched DataFrame to a list if you want to combine them later\n",
    "                    dfs.append(df_filtered)\n",
    "                    \n",
    "                    # Construct the output path for the enriched CSV file\n",
    "                    output_file_path = os.path.join(output_folder, f\"{os.path.basename(sub_file.name)}.csv\")\n",
    "\n",
    "    # Default values in case of failure\n",
    "@udf(returnType=schema)\n",
    "def trim_judgment(full_text):\n",
    "    \"\"\"\n",
    "    Trims judicial text to save LLM token.\n",
    "\n",
    "    If \"查明\" is found, it trims before this keyword and returns the rest. If not found, it trims away the first third\n",
    "    of the text, returning the remainder. Additionally, it returns the trimming type and twenty characters around the\n",
    "    trimming point for diagnosis.\n",
    "\n",
    "    Returns:\n",
    "    - trimmed_text (str): The result of the trimming operation.\n",
    "    - trimmedType (str): The method of trimming ('查明' or 'last 2/3').\n",
    "    - text_around_trimmed_point (str): Context around the point of trimming.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if full_text is None:\n",
    "        return None, \"ERROR\", \"\"\n",
    "    \n",
    "    # Search for \"查明\" from the end towards the beginning\n",
    "    index = full_text.rfind(\"查明\")\n",
    "    \n",
    "    if index != -1:\n",
    "        # If \"查明\" is found, trim the text before \"查明\"\n",
    "        trimmed_text = full_text[index + 2:]  # +2 to exclude \"查明\" itself\n",
    "        trimmedType = \"查明\"\n",
    "        # Calculate the position to start capturing text around \"查明\" safely\n",
    "        start_pos = max(0, index - 10)  # Start from 10 chars before \"查明\", if possible\n",
    "        end_pos = min(len(full_text), index + 10 + 2)  # Capture up to 10 chars after \"查明\", safely\n",
    "        text_around_trimmed_point = full_text[start_pos:end_pos]\n",
    "    else:\n",
    "        # If \"查明\" is not found, trim the first third of the text\n",
    "        one_third_length = len(full_text) // 3\n",
    "        trimmed_text = full_text[one_third_length:]\n",
    "        trimmedType = \"last 2/3\"\n",
    "        # Determine the break point and safely capture twenty characters around it\n",
    "        start_pos = max(0, one_third_length - 10)  # Start from 10 chars before the break point, if possible\n",
    "        end_pos = min(len(full_text), one_third_length + 10)  # Capture up to 10 chars after the break point, safely\n",
    "        text_around_trimmed_point = full_text[start_pos:end_pos]\n",
    "    \n",
    "    return trimmed_text, trimmedType, text_around_trimmed_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f66fc39-1427-4bd6-9be7-76ee3bcdd2ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Function: Post Request and fetch Claude Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d94bda2-a231-4a65-b0d5-53c9d4f02f3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define the schema for the UDF's return type\n",
    "schema = StructType([\n",
    "    StructField(\"response_text\", StringType(), nullable=True),\n",
    "    StructField(\"trimmedType\", StringType(), nullable=True),\n",
    "    StructField(\"text_around_trimmed_point\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "api_key = dbutils.secrets.get(scope = \"OhMyGPTAPI\", key = \"OhMyGPTAPI\")\n",
    "\n",
    "#@udf(returnType=schema)\n",
    "\n",
    "def trim_and_fetch_facts(judgment_text):\n",
    "    \n",
    "    trimmed_judgment_text, trimmedType, text_around_trimmed_point = trim_judgment(judgment_text)\n",
    "\n",
    "    url = \"https://api.ohmygpt.com/v1/messages\"\n",
    "    payload = json.dumps({\n",
    "        \"model\": \"claude-3-haiku-20240307\",\n",
    "        \"stream\": False,\n",
    "        \"system\": \"user will give you a legal judgement in Chinese, and you need to extract the kind and amount of drugs involved in the case. only reply using this format, '<kind of drug A>, <amount (in grams)>; <kind of drug B>, <amount(in grams)>'.  If you can't extract the kind of drug or amount. reply 'NA'. reply using Chinese.\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": trimmed_judgment_text}],\n",
    "        \"max_tokens\": 4096\n",
    "    })\n",
    "    headers = {\n",
    "        \"Authorization\": 'Bearer ' + api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Parse the response JSON\n",
    "            parsed_response = json.loads(response.text)\n",
    "            # Check if the expected keys/path exists\n",
    "            if \"content\" in parsed_response and len(parsed_response[\"content\"]) > 0 and \"text\" in parsed_response[\"content\"][0]:\n",
    "                response_text = parsed_response[\"content\"][0][\"text\"]\n",
    "            else:\n",
    "                raise KeyError(\"Unexpected response structure\")\n",
    "            return response_text, trimmedType, text_around_trimmed_point\n",
    "        except (requests.exceptions.RequestException, json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error on attempt {attempt+1}: {e}\")\n",
    "            # Adjust wait time if needed\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "    # Default values in case of failure\n",
    "    return \"Failed to fetch facts from API\", \"Error\", \"N/A\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecefdcb7-f128-4758-8c03-e28f707344aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Read Csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207436d5-b204-4ed9-8bb5-6c62d5648d89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ExtractFacts\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Define the set of Causes of Action to filter and sample\n",
    "causes_of_action = {\"drug_related\"}\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames, filter, and count the cases\n",
    "for cause in causes_of_action:\n",
    "    path = f\"{base_path}/{cause}/*/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Filter to get rows where TrialProcedure is '一审'\n",
    "    df_filtered = df.filter(col(\"TrialProcedure\").contains(\"一审\"))\n",
    "    \n",
    "    # Apply the UDF to the DataFrame and extract results into separate columns\n",
    "    df_processed = df_filtered.withColumn(\"processed\", trim_and_fetch_facts(df_filtered.FullText))\n",
    "\n",
    "    df_final = df_processed.select(\n",
    "        \"*\",  # Keep existing columns\n",
    "        col(\"processed.response_text\").alias(\"response_text\"),\n",
    "        col(\"processed.trimmedType\").alias(\"trimmedType\"),\n",
    "        col(\"processed.text_around_trimmed_point\").alias(\"text_around_trimmed_point\")\n",
    "    ).drop(\"processed\")  # Drop the 'processed' struct column\n",
    "    \n",
    "    # Write the modified DataFrame back to a CSV, preserving the original structure\n",
    "    output_path = f\"{base_path}/{cause}_DrugTypeAmount_March_15\"\n",
    "    df_final.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "    \n",
    "    # Count the number of cases for the current cause of action and update the dictionary\n",
    "    count = df_filtered.count()\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"{cause} modified case count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93b62fcf-82bb-49b0-ba2e-bfcb10a8eb29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test Run: read a small csv file - on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db78810f-6477-4fff-8959-96515a32bfdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CountCases\").getOrCreate()\n",
    "\n",
    "# Base path where the data is saved\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "\n",
    "# Define the set of Causes of Action to filter and sample\n",
    "causes_of_action = {\"drug_related\"}\n",
    "\n",
    "# Initialize a dictionary to hold the counts\n",
    "cases_count = {}\n",
    "\n",
    "# Iterate over each cause of action to read the saved DataFrames, filter, and count the cases\n",
    "for cause in causes_of_action:\n",
    "    # Construct the path to read the saved files for the current cause of action\n",
    "    path = f\"{base_path}/{cause}/2015_12_drug_related_judgment_data/*.csv\"\n",
    "    \n",
    "    # Read the saved data\n",
    "    df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Filter to get rows where TrialProcedure is '一审'\n",
    "    df_filtered = df.filter(col(\"TrialProcedure\").contains(\"一审\"))\n",
    "    \n",
    "    # Apply the UDF to the DataFrame and extract results into separate columns\n",
    "    df_processed = df_filtered.withColumn(\"processed\", trim_and_fetch_facts(df_filtered.FullText))\n",
    "\n",
    "    df_final = df_processed.select(\n",
    "        \"*\",  # Keep existing columns\n",
    "        \"processed.response_text\",\n",
    "        \"processed.trimmedType\",\n",
    "        \"processed.text_around_trimmed_point\"\n",
    "    ).drop(\"processed\")  # Drop the 'processed' struct column\n",
    "    \n",
    "    # Write the modified DataFrame back to a CSV, preserving the original structure\n",
    "    output_path = f\"{base_path}/{cause}_DrugTypeAmount_March_15\"\n",
    "    try:\n",
    "        df_final.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "    except Exception as e:\n",
    "            print(f\"Error writing to output path: {e}\")\n",
    "\n",
    "    \n",
    "    # Count the number of cases for the current cause of action and update the dictionary\n",
    "    count = df_filtered.count()\n",
    "    cases_count[cause] = count\n",
    "\n",
    "# Print the counts for each cause of action\n",
    "for cause, count in cases_count.items():\n",
    "    print(f\"{cause} modified case count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e1b6c9b-3fcb-4e60-b278-12de5d62eef7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test Run: Move the Data Enrichment out of Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9b4a82-952c-4ece-9cd5-ae06af7a89ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_files(base_path, cause):\n",
    "    # Define the output folder based on the base path and cause\n",
    "    output_folder = f\"{base_path}/{cause}_DrugTypeAmount_March_15\"\n",
    "    \n",
    "    # Path pattern to match CSV files for the cause of action\n",
    "    path_pattern = os.path.join(base_path, cause)\n",
    "    \n",
    "    # List to collect DataFrames (if needed)\n",
    "    dfs = []\n",
    "\n",
    "    # Using dbutils.fs.ls to list directories/files\n",
    "    files = dbutils.fs.ls(path_pattern)\n",
    "\n",
    "    for file in files:\n",
    "        # Check if the item is a directory and iterate through its contents\n",
    "        if file.isDir():\n",
    "            sub_files = dbutils.fs.ls(file.path)\n",
    "            for sub_file in sub_files:\n",
    "                if sub_file.name.endswith(\".csv\"):\n",
    "                    # Reading CSV file into DataFrame\n",
    "                    file_path = sub_file.path.replace(\"dbfs:\", \"/dbfs\")  # Convert to local file path if necessary\n",
    "                    print(file_path)\n",
    "                    df = pd.read_csv(file_path)\n",
    "\n",
    "                    \n",
    "                    # Filter rows where 'TrialProcedure' column contains '一审'\n",
    "                    df_filtered = df[df['TrialProcedure'].str.contains('一审', na=False)]\n",
    "                    if isinstance(df_filtered, pd.DataFrame):\n",
    "                        print(\"This is a Pandas DataFrame.\")\n",
    "                    else:\n",
    "                        print(\"This is not a Pandas DataFrame.\")\n",
    "                    # Apply 'trim_and_fetch_facts' function to 'FullText' column\n",
    "                    enriched_data = df_filtered['FullText'].apply(trim_and_fetch_facts)\n",
    "\n",
    "                    if df_filtered.empty:\n",
    "                        print(f\"No data after filtering for {sub_file.name}. Moving to the next file.\")\n",
    "                        continue\n",
    "\n",
    "\n",
    "                    if not isinstance(enriched_data.iloc[0], tuple) or len(enriched_data.iloc[0]) != 3:\n",
    "                        print(f\"Data structure mismatch in file: {file_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Expanding the enriched_data into separate columns\n",
    "                    df_expanded = pd.DataFrame(enriched_data.tolist(), columns=['ResponseText', 'TrimmedType', 'TextAroundTrimmedPoint'], index=df_filtered.index)\n",
    "                    df_filtered = pd.concat([df_filtered, df_expanded], axis=1)\n",
    "                    \n",
    "                    # Append the enriched DataFrame to a list if you want to combine them later\n",
    "                    dfs.append(df_filtered)\n",
    "                    \n",
    "                    # Construct the output path for the enriched CSV file\n",
    "                    output_file_path = os.path.join(output_folder, f\"{os.path.basename(sub_file.name)}.csv\")\n",
    "                    \n",
    "                    # Save the processed DataFrame to the new CSV file, ensuring the path is in \"/dbfs\" format for local IO\n",
    "                    df_filtered.to_csv(output_file_path.replace(\"dbfs:\", \"/dbfs\"), index=False)\n",
    "\n",
    "# Example usage\n",
    "base_path = \"/mnt/processed_data_criminal_case_analysis\"\n",
    "cause = \"drug_related\"\n",
    "process_files(base_path, cause)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_extraction",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
